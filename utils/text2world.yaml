run:
  max_num_steps: 30
  wandb: True
  project_name: eval-test
  baseline_dir: data/baseline_results
  log_path: none

agent:
  name: ContextEfficientAgent
  memory_size: 100
  need_goal: True
  use_parser: True

llm:
  o1-mini:
    name: gpt
    engine: o1-mini
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    max_tokens: 8192
    use_parser: False
  o3-mini:
    name: o3_mini
    engine: o3-mini
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    max_completion_tokens: 8192
    use_parser: False
  o1-preview:
    name: gpt
    engine: o1-preview
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    max_tokens: 8192
    use_parser: False
  gpt-4o-mini:
    name: gpt
    engine: gpt-4o-mini
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    max_tokens: 8192
    use_parser: False
  chatgpt-4o-latest:
    name: gpt
    engine: chatgpt-4o-latest
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    max_tokens: 8192
    use_parser: False
  gpt-4o:
    name: gpt
    engine: gpt-4o
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    max_tokens: 8192
    use_parser: False
  deepseek-v3:
    name: gpt
    engine: deepseek-chat
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    stop: 
    max_tokens: 4000
    use_parser: False
  deepseek-reasoner:
    name: deepseek_r1
    engine: deepseek-reasoner
    context_length: 8192
    use_azure: False
    retry_delays: 5
    max_retry_iters: 10000
    stop: 
    max_tokens: 4000
    use_parser: False
  gpt-4-turbo:
    name: gpt
    engine: gpt-4-turbo
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    max_tokens: 2000
    use_parser: False
  gpt-3.5-turbo-16k:
      name: gpt
      engine: gpt-3.5-turbo-16k
      context_length: 16384 # 4096
      use_azure: False
      temperature: 0.
      top_p: 1
      retry_delays: 20
      max_retry_iters: 15
      stop: "\n"
      use_parser: False
  gpt-3.5-turbo-0613:
      name: gpt
      engine: gpt-3.5-turbo-0613
      context_length: 4096
      use_azure: False
      temperature: 0.
      top_p: 1
      retry_delays: 20
      max_retry_iters: 15
      use_parser: False
  gpt-3.5-turbo-0125:
      name: gpt
      engine: gpt-3.5-turbo-0125
      context_length: 4096
      use_azure: False
      temperature: 0.
      top_p: 1
      retry_delays: 20
      max_retry_iters: 100
      use_parser: False
      max_tokens: 4096
  gpt-35-turbo: # using gpt_azure llm would need azure versin of openai key
      name: gpt_azure
      engine: gpt-35-turbo
      context_length: 4096
      use_azure: True
      temperature: 0.
      top_p: 1
      retry_delays: 20
      max_retry_iters: 15
      stop: "\n"
      use_parser: False
  text-davinci-003:
      name: gpt_azure
      engine: text-davinci-003
      context_length: 4096
      use_azure: True
      temperature: 0.
      top_p: 1
      retry_delays: 20
      max_retry_iters: 15
      stop: "\n"
      use_parser: False
  gpt-4:
      name: gpt_azure
      engine: gpt-4
      context_length: 8192
      use_azure: True
      temperature: 0.
      top_p: 1
      retry_delays: 20
      max_retry_iters: 15
      stop: "\n"
      use_parser: False
  gpt-3.5-turbo-1106:
      name: gpt
      engine: gpt-3.5-turbo-1106
      context_length: 8192
      use_azure: False
      temperature: 0.
      top_p: 1
      retry_delays: 20
      max_retry_iters: 100
      max_tokens: 2000
      use_parser: False
  claude-3.5-sonnet:
      name: claude
      engine: claude-3-5-sonnet-20241022
      temperature: 0.
      top_p: 1
      retry_delays: 10
      max_retry_iters: 15
      context_length: 100000
      xml_split:
        example: ["\n<example>\n", "</example>\n"]
        text: ["<text>\n", "</text>\n"]
        rule: ["<rule>\n", "</rule>\n"]
        system_msg: ["<system_message>\n", "</system_message>\n"]
        instruction: ["<instruction>\n", "</instruction>\n"]
        goal: ["<goal>\n", "</goal>\n"]
      use_parser: False

  llama2-7b:
      name: vllm
      engine: meta-llama/Llama-2-7b-chat-hf
      max_tokens: 2000
      temperature: 0.
      top_p: 1
      stop: 
      context_length: 4096
      dtype: float16
      ngpu: 1
      use_parser: True
  llama2-13b:
      name: vllm
      engine: meta-llama/Llama-2-13b-chat-hf
      max_tokens: 2000
      temperature: 0.
      top_p: 1
      stop:
      context_length: 4096
      dtype: float16
      ngpu: 2
      use_parser: True
  llama2-70b:
      name: vllm
      engine:  meta-llama/Llama-2-70b-chat-hf
      max_tokens: 2000
      temperature: 0.
      top_p: 1
      stop:
      context_length: 4096
      dtype: float16
      ngpu: 4
      use_parser: True
   
  llama3.1-70b:
    name: vllm
    engine: meta-llama/Llama-3.1-70B-Instruct
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    stop: 
    max_tokens: 4000
    use_parser: False
  llama3.1-8b:
    name: vllm
    engine: meta-llama/Llama-3.1-8B-Instruct
    context_length: 8192
    use_azure: False
    temperature: 0.
    top_p: 1
    retry_delays: 20
    max_retry_iters: 100
    stop: 
    max_tokens: 4000
    use_parser: False


  codellama-7b:
    name: vllm
    engine: codellama/CodeLlama-7b-Instruct-hf
    max_tokens: 2000
    temperature: 0
    top_p: 1
    stop: "\n"
    context_length: 16384
    dtype: float32
    ngpu: 4
    use_parser: True
  codellama-13b:
    name: vllm
    engine: codellama/CodeLlama-13b-Instruct-hf
    max_tokens: 2000
    temperature: 0
    top_p: 1
    stop:
    context_length: 16384
    dtype: float32
    ngpu: 4
    use_parser: True  
  codellama-34b:
    name: vllm
    engine: codellama/CodeLlama-34b-Instruct-hf
    max_tokens: 2000
    temperature: 0
    top_p: 1
    stop:
    context_length: 16384
    dtype: float32
    ngpu: 8
    use_parser: True
  codellama-70b:
    name: vllm
    engine: codellama/CodeLlama-70b-Instruct-hf
    max_tokens: 2000
    temperature: 0
    top_p: 1
    stop: "\n"
    context_length: 16384
    dtype: float32
    ngpu: 4
    use_parser: True

env: none
